{
  "django__django-16662": {
    "model_name_or_path": "gpt-5-mini",
    "instance_id": "django__django-16662",
    "model_patch": "diff --git a/django/db/migrations/writer.py b/django/db/migrations/writer.py\nindex a59f0c8dcb..42059e9ae6 100644\n--- a/django/db/migrations/writer.py\n+++ b/django/db/migrations/writer.py\n@@ -148,10 +148,13 @@ class MigrationWriter:\n         for dependency in self.migration.dependencies:\n             if dependency[0] == \"__setting__\":\n                 dependencies.append(\n-                    \"        migrations.swappable_dependency(settings.%s),\"\n-                    % dependency[1]\n-                )\n-                imports.add(\"from django.conf import settings\")\n+# Sort imports by whether they are plain imports or from-imports,\n+          # then by the package/module name to keep a stable order.\n+          def _import_sort_key(i):\n+              parts = i.split()\n+              return (0, parts[1]) if parts[0] == \"import\" else (1, parts[1])\n+          sorted_imports = sorted(imports, key=_import_sort_key)\n+          items[\"imports\"] = \"\\n\".join(sorted_imports) + \"\\n\" if imports else \"\"\n             else:\n                 dependencies.append(\"        %s,\" % self.serialize(dependency)[0])\n         items[\"dependencies\"] = \"\\n\".join(dependencies) + \"\\n\" if dependencies else \"\""
  },
  "sympy__sympy-24213": {
    "model_name_or_path": "gpt-5-mini",
    "instance_id": "sympy__sympy-24213",
    "model_patch": "diff --git a/sympy/physics/units/unitsystem.py b/sympy/physics/units/unitsystem.py\nindex a436880c4b..2613b10a67 100644\n--- a/sympy/physics/units/unitsystem.py\n+++ b/sympy/physics/units/unitsystem.py\n@@ -169,18 +169,18 @@ def _collect_factor_and_dimension(self, expr):\n             exp_factor, exp_dim = self._collect_factor_and_dimension(expr.exp)\n             if self.get_dimension_system().is_dimensionless(exp_dim):\n                 exp_dim = 1\n-            return factor ** exp_factor, dim ** (exp_factor * exp_dim)\n-        elif isinstance(expr, Add):\n-            factor, dim = self._collect_factor_and_dimension(expr.args[0])\n-            for addend in expr.args[1:]:\n-                addend_factor, addend_dim = \\\n-                    self._collect_factor_and_dimension(addend)\n-                if dim != addend_dim:\n-                    raise ValueError(\n-                        'Dimension of \"{}\" is {}, '\n-                        'but it should be {}'.format(\n-                            addend, addend_dim, dim))\n-                factor += addend_factor\n+elif isinstance(expr, Add):\n+                factor, dim = self._collect_factor_and_dimension(expr.args[0])\n+                for addend in expr.args[1:]:\n+                    addend_factor, addend_dim = \\\n+                        self._collect_factor_and_dimension(addend)\n+                    if not self.get_dimension_system().equivalent_dims(dim, addend_dim):\n+                        raise ValueError(\n+                            'Dimension of \"{}\" is {}, '\n+                            'but it should be {}'.format(\n+                                addend, addend_dim, dim))\n+                    factor += addend_factor\n+                return factor, dim\n             return factor, dim\n         elif isinstance(expr, Derivative):\n             factor, dim = self._collect_factor_and_dimension(expr.args[0])"
  },
  "django__django-7530": {
    "model_name_or_path": "gpt-5-mini",
    "instance_id": "django__django-7530",
    "model_patch": "diff --git a/django/core/management/commands/makemigrations.py b/django/core/management/commands/makemigrations.py\nindex 0b39536f5b..b579081807 100644\n--- a/django/core/management/commands/makemigrations.py\n+++ b/django/core/management/commands/makemigrations.py\n@@ -86,27 +86,19 @@ class Command(BaseCommand):\n                 apps.get_app_config(app_label)\n             except LookupError:\n                 bad_app_labels.add(app_label)\n-        if bad_app_labels:\n-            for app_label in bad_app_labels:\n-                self.stderr.write(\"App '%s' could not be found. Is it in INSTALLED_APPS?\" % app_label)\n-            sys.exit(2)\n+# Raise an error if any migrations are applied before their dependencies.\n+    consistency_check_labels = set(config.label for config in apps.get_app_configs())\n+    # Non-default databases are only checked if database routers used.\n+    aliases_to_check = connections if settings.DATABASE_ROUTERS else [DEFAULT_DB_ALIAS]\n+    for alias in sorted(aliases_to_check):\n+        connection = connections[alias]\n+        if (connection.settings_dict['ENGINE'] != 'django.db.backends.dummy' and any(\n+                # At least one model must be migrated to the database.\n+                router.allow_migrate(connection.alias, app_label, model_name=model._meta.object_name)\n+                for app_label in consistency_check_labels\n+                for model in apps.get_app_config(app_label).get_models()\n+        )):\n \n-        # Load the current graph state. Pass in None for the connection so\n-        # the loader doesn't try to resolve replaced migrations from DB.\n-        loader = MigrationLoader(None, ignore_no_migrations=True)\n-\n-        # Raise an error if any migrations are applied before their dependencies.\n-        consistency_check_labels = set(config.label for config in apps.get_app_configs())\n-        # Non-default databases are only checked if database routers used.\n-        aliases_to_check = connections if settings.DATABASE_ROUTERS else [DEFAULT_DB_ALIAS]\n-        for alias in sorted(aliases_to_check):\n-            connection = connections[alias]\n-            if (connection.settings_dict['ENGINE'] != 'django.db.backends.dummy' and any(\n-                    # At least one model must be migrated to the database.\n-                    router.allow_migrate(connection.alias, app_label, model_name=model._meta.object_name)\n-                    for app_label in consistency_check_labels\n-                    for model in apps.get_models(app_label)\n-            )):\n                 loader.check_consistent_history(connection)\n \n         # Before anything else, see if there's conflicting apps and drop out"
  },
  "django__django-14053": {
    "model_name_or_path": "gpt-5-mini",
    "instance_id": "django__django-14053",
    "model_patch": "diff --git a/django/contrib/staticfiles/storage.py b/django/contrib/staticfiles/storage.py\nindex 494890cc23..3f59b68b1f 100644\n--- a/django/contrib/staticfiles/storage.py\n+++ b/django/contrib/staticfiles/storage.py\n@@ -200,7 +200,7 @@ class HashedFilesMixin:\n \n         return converter\n \n-    def post_process(self, paths, dry_run=False, **options):\n+def post_process(self, paths, dry_run=False, **options):\n         \"\"\"\n         Post process the given dictionary of files (called from collectstatic).\n \n@@ -211,10 +211,12 @@ class HashedFilesMixin:\n         2. adjusting files which contain references to other files so they\n            refer to the cache-busting filenames.\n \n-        If either of these are performed on a file, then that file is considered\n-        post-processed.\n+        This implementation performs multiple passes to ensure nested references\n+        stabilize, but only yields each original filename once with its final\n+        post-processed name so callers (collectstatic and subclasses) aren't\n+        given duplicate work or misleading counts.\n         \"\"\"\n-        # don't even dare to process the files if we're in dry run mode\n+        # don't process in dry-run mode\n         if dry_run:\n             return\n \n@@ -226,28 +228,42 @@ class HashedFilesMixin:\n             path for path in paths\n             if matches_patterns(path, self._patterns)\n         ]\n-        # Do a single pass first. Post-process all files once, then repeat for\n-        # adjustable files.\n+\n+        # Keep track of the current name for each original path (start identity)\n+        current_names = {original: original for original in paths}\n+        any_processed = False\n+\n+        # Run the initial full pass (as before) but capture results instead of yielding\n         for name, hashed_name, processed, _ in self._post_process(paths, adjustable_paths, hashed_files):\n-            yield name, hashed_name, processed\n+            current_names[name] = hashed_name\n+            any_processed = any_processed or bool(processed)\n \n+        # Narrow to adjustable paths for subsequent passes\n         paths = {path: paths[path] for path in adjustable_paths}\n \n+        # Repeat passes until no substitutions occur or max passes reached\n         for i in range(self.max_post_process_passes):\n             substitutions = False\n             for name, hashed_name, processed, subst in self._post_process(paths, adjustable_paths, hashed_files):\n-                yield name, hashed_name, processed\n+                current_names[name] = hashed_name\n                 substitutions = substitutions or subst\n-\n+                any_processed = any_processed or bool(processed)\n             if not substitutions:\n                 break\n-\n-        if substitutions:\n+        else:\n+            # Max passes exceeded; yield the same error as before.\n             yield 'All', None, RuntimeError('Max post-process passes exceeded.')\n \n         # Store the processed paths\n         self.hashed_files.update(hashed_files)\n \n+        # Yield each original exactly once with its final name if it changed.\n+        for original, final_name in current_names.items():\n+            if final_name != original:\n+                yield original, final_name, True\n+\n+        if not any_processed:\n+            return\n     def _post_process(self, paths, adjustable_paths, hashed_files):\n         # Sort the files by directory level\n         def path_level(name):"
  },
  "scikit-learn__scikit-learn-26323": {
    "model_name_or_path": "gpt-5-mini",
    "instance_id": "scikit-learn__scikit-learn-26323",
    "model_patch": "diff --git a/sklearn/compose/_column_transformer.py b/sklearn/compose/_column_transformer.py\nindex 4eddd12b1..589782ea6 100644\n--- a/sklearn/compose/_column_transformer.py\n+++ b/sklearn/compose/_column_transformer.py\n@@ -225,44 +225,7 @@ class ColumnTransformer(TransformerMixin, _BaseComposition):\n         ],\n         \"sparse_threshold\": [Interval(Real, 0, 1, closed=\"both\")],\n         \"n_jobs\": [Integral, None],\n-        \"transformer_weights\": [dict, None],\n-        \"verbose\": [\"verbose\"],\n-        \"verbose_feature_names_out\": [\"boolean\"],\n-    }\n-\n-    def __init__(\n-        self,\n-        transformers,\n-        *,\n-        remainder=\"drop\",\n-        sparse_threshold=0.3,\n-        n_jobs=None,\n-        transformer_weights=None,\n-        verbose=False,\n-        verbose_feature_names_out=True,\n-    ):\n-        self.transformers = transformers\n-        self.remainder = remainder\n-        self.sparse_threshold = sparse_threshold\n-        self.n_jobs = n_jobs\n-        self.transformer_weights = transformer_weights\n-        self.verbose = verbose\n-        self.verbose_feature_names_out = verbose_feature_names_out\n-\n-    @property\n-    def _transformers(self):\n-        \"\"\"\n-        Internal list of transformer only containing the name and\n-        transformers, dropping the columns. This is for the implementation\n-        of get_params via BaseComposition._get_params which expects lists\n-        of tuples of len 2.\n-        \"\"\"\n-        try:\n-            return [(name, trans) for name, trans, _ in self.transformers]\n-        except (TypeError, ValueError):\n-            return self.transformers\n-\n-    @_transformers.setter\n+@_transformers.setter\n     def _transformers(self, value):\n         try:\n             self.transformers = [\n@@ -300,6 +263,24 @@ class ColumnTransformer(TransformerMixin, _BaseComposition):\n             )\n             if trans not in {\"passthrough\", \"drop\"}\n         )\n+        for trans in transformers:\n+            _safe_set_output(trans, transform=transform)\n+\n+                self.transformers, getattr(self, \"transformers_\", [])\n+            )\n+            if trans not in {\"passthrough\", \"drop\"}\n+        )\n+        for trans in transformers:\n+            _safe_set_output(trans, transform=transform)\n+\n+        # Also set output on remainder if it's an estimator (not 'drop' or 'passthrough')\n+        if getattr(self, \"remainder\", None) not in {\"passthrough\", \"drop\"}:\n+            _safe_set_output(self.remainder, transform=transform)\n+\n+        return self\n+            )\n+            if trans not in {\"passthrough\", \"drop\"}\n+        )\n         for trans in transformers:\n             _safe_set_output(trans, transform=transform)"
  },
  "sphinx-doc__sphinx-9658": {
    "model_name_or_path": "gpt-5-mini",
    "instance_id": "sphinx-doc__sphinx-9658",
    "model_patch": "diff --git a/doc/conf.py b/doc/conf.py\nindex 220774b7f..2a968b0b5 100644\n--- a/doc/conf.py\n+++ b/doc/conf.py\n@@ -116,26 +116,80 @@ locale_dirs = ['locale/']\n gettext_compact = False\n \n \n-# -- Extension interface -------------------------------------------------------\n-\n-from sphinx import addnodes  # noqa\n-\n-event_sig_re = re.compile(r'([a-zA-Z-]+)\\s*\\((.*)\\)')\n-\n-\n-def parse_event(env, sig, signode):\n-    m = event_sig_re.match(sig)\n-    if not m:\n-        signode += addnodes.desc_name(sig, sig)\n-        return sig\n-    name, args = m.groups()\n-    signode += addnodes.desc_name(name, name)\n-    plist = addnodes.desc_parameterlist()\n-    for arg in args.split(','):\n-        arg = arg.strip()\n-        plist += addnodes.desc_parameter(arg, arg)\n-    signode += plist\n-    return name\n+def _resolve_mocked_base_class(modpath):\n+    \"\"\"\n+    Try to import 'modpath' and return an attribute named 'Module' if present.\n+    Returns None if resolution fails.\n+    \"\"\"\n+    try:\n+        mod = __import__(modpath, fromlist=['Module'])\n+        if hasattr(mod, 'Module'):\n+            return getattr(mod, 'Module')\n+    except Exception:\n+        return None\n+    return None\n+\n+def _handle_autodoc_bases(app, name, obj, options, bases):\n+    \"\"\"\n+    Post-process autodoc 'bases' in-place to fix truncated module-only names\n+    (e.g. 'torch.nn.') that can appear when modules are mocked.\n+    - If a base is a string and ends with a dot, try to import \"<module>.Module\".\n+    - If successful, replace the base with the resolved class object so Sphinx prints its proper name.\n+    - Otherwise replace with a readable fallback string like \"torch.nn.Module\".\n+    \"\"\"\n+    new_bases = []\n+    for base in bases:\n+        try:\n+            # If Sphinx provided a string like 'torch.nn.' or similar\n+            if isinstance(base, str):\n+                b = base\n+                if b.endswith('.'):\n+                    modpath = b.rstrip('.')\n+                    resolved = _resolve_mocked_base_class(modpath)\n+                    if resolved is not None:\n+                        new_bases.append(resolved)\n+                        continue\n+                    # fallback: show \"modpath.Module\" as a readable string\n+                    new_bases.append(f\"{modpath}.Module\")\n+                    continue\n+                # not truncated string -> keep as-is\n+                new_bases.append(base)\n+                continue\n+            # If it's an object, keep it if it looks like a real type\n+            if hasattr(base, '__module__') and hasattr(base, '__name__'):\n+                new_bases.append(base)\n+            else:\n+                # unexpected mock-like object: fall back to its string repr\n+                new_bases.append(str(base))\n+        except Exception:\n+            # conservative fallback on any error\n+            new_bases.append(base)\n+    # replace contents in-place (Sphinx expects bases mutated)\n+    bases[:] = new_bases\n+\n+def setup(app):\n+    from sphinx.ext.autodoc import cut_lines\n+    from sphinx.util.docfields import GroupedField\n+    app.connect('autodoc-process-docstring', cut_lines(4, what=['module']))\n+    # Register our handler to fix truncated mocked base names\n+    app.connect('autodoc-process-bases', _handle_autodoc_bases)\n+    app.add_object_type('confval', 'confval',\n+                        objname='configuration value',\n+                        indextemplate='pair: %s; configuration value')\n+    app.add_object_type('setuptools-confval', 'setuptools-confval',\n+                        objname='setuptools configuration value',\n+                        indextemplate='pair: %s; setuptools configuration value')\n+    fdesc = GroupedField('parameter', label='Parameters',\n+                         names=['param'], can_collapse=True)\n+    app.add_object_type('event', 'event', 'pair: %s; event', parse_event,\n+                        doc_field_types=[fdesc])\n+\n+    # workaround for RTD\n+    from sphinx.util import logging\n+    logger = logging.getLogger(__name__)\n+    app.info = lambda *args, **kwargs: logger.info(*args, **kwargs)\n+    app.warn = lambda *args, **kwargs: logger.warning(*args, **kwargs)\n+    app.debug = lambda *args, **kwargs: logger.debug(*args, **kwargs)\n \n \n def setup(app):"
  },
  "psf__requests-2931": {
    "model_name_or_path": "gpt-5-mini",
    "instance_id": "psf__requests-2931",
    "model_patch": "diff --git a/requests/utils.py b/requests/utils.py\nindex 132cd2b..d6d48c5 100644\n--- a/requests/utils.py\n+++ b/requests/utils.py\n@@ -654,8 +654,6 @@ def guess_json_utf(data):\n             return 'utf-32-le'\n         # Did not detect a valid UTF-32 ascii-range character\n     return None\n-\n-\n def prepend_scheme_if_needed(url, new_scheme):\n     '''Given a URL that may or may not have a scheme, prepend the given scheme.\n     Does not replace a present scheme with the one provided as an argument.'''\n@@ -693,6 +691,20 @@ def to_native_string(string, encoding='ascii'):\n \n     if isinstance(string, builtin_str):\n         out = string\n+    else:\n+        if is_py2:\n+            out = string.encode(encoding)\n+        else:\n+            # Python 3: bytes/bytearray must be decoded to str. Try the requested\n+            # encoding first; if that fails (e.g. bytes are UTF-8 but encoding\n+\n+            else:\n+                out = str(string)\n+\n+    return out\n+\n+    return out\n+        out = string\n     else:\n         if is_py2:\n             out = string.encode(encoding)"
  },
  "django__django-11179": {
    "model_name_or_path": "gpt-5-mini",
    "instance_id": "django__django-11179",
    "model_patch": "diff --git a/django/db/models/deletion.py b/django/db/models/deletion.py\nindex 92fe4affb9..1a8a02209a 100644\n--- a/django/db/models/deletion.py\n+++ b/django/db/models/deletion.py\n@@ -262,21 +262,26 @@ class Collector:\n     def delete(self):\n         # sort instance collections\n         for model, instances in self.data.items():\n-            self.data[model] = sorted(instances, key=attrgetter(\"pk\"))\n-\n-        # if possible, bring the models in an order suitable for databases that\n-        # don't support transactions or cannot defer constraint checks until the\n-        # end of a transaction.\n-        self.sort()\n-        # number of objects deleted for each model label\n-        deleted_counter = Counter()\n-\n-        # Optimize for the case with a single obj and no dependencies\n-        if len(self.data) == 1 and len(instances) == 1:\n-            instance = list(instances)[0]\n-            if self.can_fast_delete(instance):\n-                with transaction.mark_for_rollback_on_error():\n-                    count = sql.DeleteQuery(model).delete_batch([instance.pk], self.using)\n+# if possible, bring the models in an order suitable for databases that\n+          # don't support transactions or cannot defer constraint checks until the\n+          # end of a transaction.\n+          self.sort()\n+          # number of objects deleted for each model label\n+          deleted_counter = Counter()\n+    \n+          # Optimize for the case with a single obj and no dependencies\n+          if len(self.data) == 1 and len(instances) == 1:\n+              instance = list(instances)[0]\n+              if self.can_fast_delete(instance):\n+                  with transaction.mark_for_rollback_on_error():\n+                      count = sql.DeleteQuery(model).delete_batch([instance.pk], self.using)\n+                  # Clear the primary key on the instance to reflect that it has been deleted,\n+                  # matching Django's behavior for delete() on instances without dependencies.\n+                  try:\n+                      setattr(instance, model._meta.pk.attname, None)\n+                  except Exception:\n+                      instance.pk = None\n+\n                 return count, {model._meta.label: count}\n \n         with transaction.atomic(using=self.using, savepoint=False):"
  },
  "django__django-10973": {
    "model_name_or_path": "gpt-5-mini",
    "instance_id": "django__django-10973",
    "model_patch": "diff --git a/django/db/backends/postgresql/client.py b/django/db/backends/postgresql/client.py\nindex 7fca6eff30..b7c089ee96 100644\n--- a/django/db/backends/postgresql/client.py\n+++ b/django/db/backends/postgresql/client.py\n@@ -37,28 +37,36 @@ class DatabaseClient(BaseDatabaseClient):\n         temp_pgpass = None\n         sigint_handler = signal.getsignal(signal.SIGINT)\n         try:\n+            env = None\n             if passwd:\n-                # Create temporary .pgpass file.\n-                temp_pgpass = NamedTemporaryFile(mode='w+')\n                 try:\n-                    print(\n-                        _escape_pgpass(host) or '*',\n-                        str(port) or '*',\n-                        _escape_pgpass(dbname) or '*',\n-                        _escape_pgpass(user) or '*',\n-                        _escape_pgpass(passwd),\n-                        file=temp_pgpass,\n-                        sep=':',\n-                        flush=True,\n-                    )\n-                    os.environ['PGPASSFILE'] = temp_pgpass.name\n+                    # Prefer passing the password via the PGPASSWORD environment variable.\n+                    env = os.environ.copy()\n+                    env['PGPASSWORD'] = passwd\n                 except UnicodeEncodeError:\n-                    # If the current locale can't encode the data, let the\n-                    # user input the password manually.\n-                    pass\n+                    # Fallback to temporary .pgpass file if the current locale can't encode the data.\n+                    temp_pgpass = NamedTemporaryFile(mode='w+')\n+                    try:\n+                        print(\n+                            _escape_pgpass(host) or '*',\n+                            str(port) or '*',\n+                            _escape_pgpass(dbname) or '*',\n+                            _escape_pgpass(user) or '*',\n+                            _escape_pgpass(passwd),\n+                            file=temp_pgpass,\n+                            sep=':',\n+                            flush=True,\n+                        )\n+                        os.environ['PGPASSFILE'] = temp_pgpass.name\n+                    except UnicodeEncodeError:\n+                        # If still not possible, let the user input the password manually.\n+                        pass\n             # Allow SIGINT to pass to psql to abort queries.\n             signal.signal(signal.SIGINT, signal.SIG_IGN)\n-            subprocess.check_call(args)\n+            if env is not None:\n+                subprocess.run(args, env=env, check=True)\n+            else:\n+                subprocess.check_call(args)\n         finally:\n             # Restore the original SIGINT handler.\n             signal.signal(signal.SIGINT, sigint_handler)\n@@ -67,5 +75,13 @@ class DatabaseClient(BaseDatabaseClient):\n                 if 'PGPASSFILE' in os.environ:  # unit tests need cleanup\n                     del os.environ['PGPASSFILE']\n \n+    def runshell(self):\n+        DatabaseClient.runshell_db(self.connection.get_connection_params())\n+            signal.signal(signal.SIGINT, sigint_handler)\n+            if temp_pgpass:\n+                temp_pgpass.close()\n+                if 'PGPASSFILE' in os.environ:  # unit tests need cleanup\n+                    del os.environ['PGPASSFILE']\n+\n     def runshell(self):\n         DatabaseClient.runshell_db(self.connection.get_connection_params())"
  },
  "django__django-12406": {
    "model_name_or_path": "gpt-5-mini",
    "instance_id": "django__django-12406",
    "model_patch": "diff --git a/django/forms/models.py b/django/forms/models.py\nindex dafc11f995..e3bcdfae2c 100644\n--- a/django/forms/models.py\n+++ b/django/forms/models.py\n@@ -1127,27 +1127,22 @@ class InlineForeignKeyField(Field):\n \n \n class ModelChoiceIteratorValue:\n-    def __init__(self, value, instance):\n-        self.value = value\n-        self.instance = instance\n-\n-    def __str__(self):\n-        return str(self.value)\n-\n-    def __eq__(self, other):\n-        if isinstance(other, ModelChoiceIteratorValue):\n-            other = other.value\n-        return self.value == other\n-\n-\n class ModelChoiceIterator:\n     def __init__(self, field):\n         self.field = field\n         self.queryset = field.queryset\n \n     def __iter__(self):\n+        # Don't include the empty choice for RadioSelect when the field is required\n+        # (RadioSelect has an inherent unfilled state, so the blank option is unnecessary\n+        # and can be confusing when the model disallows blank).\n+        widget_cls = getattr(self.field.widget, '__class__', None)\n+        include_empty = True\n         if self.field.empty_label is not None:\n-            yield (\"\", self.field.empty_label)\n+            if getattr(widget_cls, '__name__', None) == 'RadioSelect' and getattr(self.field, 'required', False):\n+                include_empty = False\n+            if include_empty:\n+                yield (\"\", self.field.empty_label)\n         queryset = self.queryset\n         # Can't use iterator() when queryset uses prefetch_related()\n         if not queryset._prefetch_related_lookups:\n@@ -1169,6 +1164,17 @@ class ModelChoiceIterator:\n             ModelChoiceIteratorValue(self.field.prepare_value(obj), obj),\n             self.field.label_from_instance(obj),\n         )\n+        # and __len__() won't be called.\n+        return self.queryset.count() + (1 if self.field.empty_label is not None else 0)\n+\n+    def __bool__(self):\n+        return self.field.empty_label is not None or self.queryset.exists()\n+\n+    def choice(self, obj):\n+        return (\n+            ModelChoiceIteratorValue(self.field.prepare_value(obj), obj),\n+            self.field.label_from_instance(obj),\n+        )\n \n \n class ModelChoiceField(ChoiceField):"
  },
  "psf__requests-1921": {
    "model_name_or_path": "gpt-5-mini",
    "instance_id": "psf__requests-1921",
    "model_patch": "diff --git a/docs/user/advanced.rst b/docs/user/advanced.rst\nindex 80c1e6a..90caebc 100644\n--- a/docs/user/advanced.rst\n+++ b/docs/user/advanced.rst\n@@ -21,10 +21,15 @@ Let's persist some cookies across requests::\n \n     s.get('http://httpbin.org/cookies/set/sessioncookie/123456789')\n     r = s.get(\"http://httpbin.org/cookies\")\n+Any dictionaries that you pass to a request method will be merged with the session-level values that are set. The method-level parameters override session parameters.\n+\n+.. admonition:: Remove a Value From a Dict Parameter\n+\n+    Sometimes you'll want to omit session-level keys from a dict parameter. To do this, you simply set that key's value to ``None`` in the method-level parameter. It will automatically be omitted.\n \n-    print(r.text)\n-    # '{\"cookies\": {\"sessioncookie\": \"123456789\"}}'\n+    .. note::\n \n+        Setting a session object's default header (for example ``session.headers['Accept-Encoding']``) to ``None`` will not omit the header \u2014 Requests will stringify ``None`` and send ``Accept-Encoding: None``. To prevent a session header from being sent, remove the key from ``session.headers`` (for example ``del session.headers['Accept-Encoding']`` or ``session.headers.pop('Accept-Encoding', None)``). To omit a header for a single request without mutating the session, pass a copy of the headers with the key removed.\n \n Sessions can also be used to provide default data to the request methods. This\n is done by providing data to the properties on a Session object::"
  },
  "django__django-14011": {
    "model_name_or_path": "gpt-5-mini",
    "instance_id": "django__django-14011",
    "model_patch": "diff --git a/django/core/servers/basehttp.py b/django/core/servers/basehttp.py\nindex 14dab517c9..d378e1da3b 100644\n--- a/django/core/servers/basehttp.py\n+++ b/django/core/servers/basehttp.py\n@@ -4,9 +4,6 @@ HTTP server that implements the Python WSGI protocol (PEP 333, rev 1.21).\n Based on wsgiref.simple_server which is part of the standard library since 2.5.\n \n This is a simple server for use in testing or debugging Django apps. It hasn't\n-been reviewed for security issues. DON'T USE IT FOR PRODUCTION USE!\n-\"\"\"\n-\n import logging\n import socket\n import socketserver\n@@ -14,6 +11,9 @@ import sys\n from wsgiref import simple_server\n \n from django.core.exceptions import ImproperlyConfigured\n+from django.core.handlers.wsgi import LimitedStream\n+from django.core.wsgi import get_wsgi_application\n+\n from django.core.handlers.wsgi import LimitedStream\n from django.core.wsgi import get_wsgi_application\n from django.utils.module_loading import import_string\n@@ -50,20 +50,74 @@ def get_internal_wsgi_application():\n         ) from err\n \n \n-def is_broken_pipe_error():\n-    exc_type, _, _ = sys.exc_info()\n+request_queue_size = 10\n+\n+    def __init__(self, *args, ipv6=False, allow_reuse_address=True, **kwargs):\n+        if ipv6:\n+            self.address_family = socket.AF_INET6\n+        self.allow_reuse_address = allow_reuse_address\n+        super().__init__(*args, **kwargs)\n     return issubclass(exc_type, (\n         BrokenPipeError,\n         ConnectionAbortedError,\n         ConnectionResetError,\n     ))\n+class ThreadedWSGIServer(socketserver.ThreadingMixIn, WSGIServer):\n+    \"\"\"\n+    A threaded version of the WSGIServer that ensures per-request threads close\n+    Django DB connections when they finish to avoid leaving connections open that\n+    interfere with test database teardown.\n+    \"\"\"\n+    # Let worker threads be daemons by default to reduce shutdown/join window.\n+    daemon_threads = True\n \n+    def __init__(self, *args, connections_override=None, **kwargs):\n+        # connections_override is a dict of connections (used by tests) or None.\n+        self.connections_override = connections_override\n+        # Event used by tests to observe that a request thread closed connections.\n+        self._connections_closed = threading.Event()\n+        super().__init__(*args, **kwargs)\n \n-class WSGIServer(simple_server.WSGIServer):\n-    \"\"\"BaseHTTPServer that implements the Python WSGI protocol\"\"\"\n+    def _close_connections(self):\n+        # Close the override connections if provided (used in tests), otherwise\n+        # close all Django connections for the current thread.\n+        if self.connections_override is not None:\n+            for conn in self.connections_override.values():\n+                try:\n+                    conn.close()\n+                except Exception:\n+                    # Be defensive: don't let connection close errors break request cleanup.\n+                    pass\n+        else:\n+            try:\n+                connections.close_all()\n+            except Exception:\n+                pass\n+        # Mark that we have attempted to close connections (for test observability).\n+        try:\n+            self._connections_closed.set()\n+        except Exception:\n+            pass\n \n-    request_queue_size = 10\n+    def close_request(self, request):\n+        \"\"\"\n+        Called to clean up an individual request \u2014 ensure DB connections from\n+        the handling thread are closed before delegating to the base implementation.\n+        \"\"\"\n+        # Ensure thread-local DB connections are closed immediately.\n+        self._close_connections()\n+        super().close_request(request)\n \n+    def process_request_thread(self, request, client_address):\n+        \"\"\"\n+        Ensure connections are closed even if finish_request/process_request\n+        raises in the request thread.\n+        \"\"\"\n+        try:\n+            super().process_request_thread(request, client_address)\n+        finally:\n+            # Best-effort close to avoid leaving connections open across threads.\n+            self._close_connections()\n     def __init__(self, *args, ipv6=False, allow_reuse_address=True, **kwargs):\n         if ipv6:\n             self.address_family = socket.AF_INET6"
  },
  "astropy__astropy-7166": {
    "model_name_or_path": "gpt-5-mini",
    "instance_id": "astropy__astropy-7166",
    "model_patch": "diff --git a/astropy/utils/misc.py b/astropy/utils/misc.py\nindex de3fb01d1..8550ef478 100644\n--- a/astropy/utils/misc.py\n+++ b/astropy/utils/misc.py\n@@ -488,12 +488,6 @@ def did_you_mean(s, candidates, n=3, cutoff=0.8, fix=None):\n             matches = matches[0]\n         else:\n             matches = (', '.join(matches[:-1]) + ' or ' +\n-                       matches[-1])\n-        return 'Did you mean {0}?'.format(matches)\n-\n-    return ''\n-\n-\n class InheritDocstrings(type):\n     \"\"\"\n     This metaclass makes methods of a class automatically have their\n@@ -528,13 +522,63 @@ class InheritDocstrings(type):\n                 not key.startswith('_'))\n \n         for key, val in dct.items():\n-            if (inspect.isfunction(val) and\n-                is_public_member(key) and\n-                val.__doc__ is None):\n-                for base in cls.__mro__[1:]:\n-                    super_method = getattr(base, key, None)\n-                    if super_method is not None:\n-                        val.__doc__ = super_method.__doc__\n+            if not is_public_member(key):\n+                continue\n+\n+            # Regular functions (instance methods)\n+            if inspect.isfunction(val):\n+                if val.__doc__ is None:\n+                    for base in cls.__mro__[1:]:\n+                        super_method = getattr(base, key, None)\n+                        if super_method is not None:\n+                            val.__doc__ = getattr(super_method, '__doc__', None)\n+                            if val.__doc__:\n+                                break\n+\n+            # Properties: inherit docstring into the fget if missing\n+            elif isinstance(val, property):\n+                fget = val.fget\n+                if fget is not None and fget.__doc__ is None:\n+                    for base in cls.__mro__[1:]:\n+                        super_attr = getattr(base, key, None)\n+                        if super_attr is None:\n+                            continue\n+                        if isinstance(super_attr, property):\n+                            doc = super_attr.__doc__ or getattr(getattr(super_attr, 'fget', None), '__doc__', None)\n+                        else:\n+                            doc = getattr(super_attr, '__doc__', None)\n+                        if doc:\n+                            fget.__doc__ = doc\n+                            break\n+\n+            # classmethod and staticmethod wrap the underlying function in __func__\n+            elif isinstance(val, (classmethod, staticmethod)):\n+                func = getattr(val, '__func__', None)\n+                if func is not None and func.__doc__ is None:\n+                    for base in cls.__mro__[1:]:\n+                        super_method = getattr(base, key, None)\n+                        if super_method is None:\n+                            continue\n+                        if isinstance(super_method, (classmethod, staticmethod)):\n+                            super_func = getattr(super_method, '__func__', None)\n+                            doc = getattr(super_func, '__doc__', None) if super_func is not None else None\n+                        else:\n+                            doc = getattr(super_method, '__doc__', None)\n+                        if doc:\n+                            func.__doc__ = doc\n+                            break\n+\n+        super().__init__(name, bases, dct)\n+\n+        super().__init__(name, bases, dct)\n+                            if doc:\n+                                func.__doc__ = doc\n+                                break\n+\n+        super().__init__(name, bases, dct)\n+                                break\n+\n+        super().__init__(name, bases, dct)\n                         break\n \n         super().__init__(name, bases, dct)"
  },
  "django__django-16631": {
    "model_name_or_path": "gpt-5-mini",
    "instance_id": "django__django-16631",
    "model_patch": "diff --git a/0001-fix-secret-key-fallback.diff b/0001-fix-secret-key-fallback.diff\nnew file mode 100644\nindex 0000000000..4ceff87cfa\n--- /dev/null\n+++ b/0001-fix-secret-key-fallback.diff\n@@ -0,0 +1,54 @@\n+*** Begin Patch\n+*** Update File: django/contrib/auth/__init__.py\n+@@\n+-                    if hasattr(user, \"get_session_auth_hash\"):\n+-                        session_hash = request.session.get(HASH_SESSION_KEY)\n+-                        if not session_hash:\n+-                            session_hash_verified = False\n+-                        else:\n+-                            session_auth_hash = user.get_session_auth_hash()\n+-                            session_hash_verified = constant_time_compare(\n+-                                session_hash, session_auth_hash\n+-                            )\n+-                        if not session_hash_verified:\n+-                            request.session.flush()\n+-                            user = None\n++                    if hasattr(user, \"get_session_auth_hash\"):\n++                        session_hash = request.session.get(HASH_SESSION_KEY)\n++                        if not session_hash:\n++                            session_hash_verified = False\n++                        else:\n++                            session_auth_hash = user.get_session_auth_hash()\n++                            session_hash_verified = constant_time_compare(\n++                                session_hash, session_auth_hash\n++                            )\n++                        if not session_hash_verified:\n++                            # If the current secret does not verify the session,\n++                            # try with the fallback secrets and stop when a match\n++                            # is found. If a fallback matches, upgrade the session\n++                            # to use the current SECRET_KEY-derived hash.\n++                            if session_hash and any(\n++                                constant_time_compare(session_hash, fallback_auth_hash)\n++                                for fallback_auth_hash in user.get_session_auth_fallback_hash()\n++                            ):\n++                                request.session.cycle_key()\n++                                request.session[HASH_SESSION_KEY] = session_auth_hash\n++                            else:\n++                                request.session.flush()\n++                                user = None\n+@@\n+-                    if not session_hash_verified:\n+-                        request.session.flush()\n+-                        user = None\n++                    if not session_hash_verified:\n++                        # Async equivalent: try fallbacks then upgrade or flush.\n++                        if session_hash and any(\n++                            constant_time_compare(session_hash, fallback_auth_hash)\n++                            for fallback_auth_hash in user.get_session_auth_fallback_hash()\n++                        ):\n++                            await request.session.acycle_key()\n++                            await request.session.aset(HASH_SESSION_KEY, session_auth_hash)\n++                        else:\n++                            await request.session.aflush()\n++                            user = None\n+*** End Patch\ndiff --git a/tests/test_secret_key_fallback.py b/tests/test_secret_key_fallback.py\nnew file mode 100644\nindex 0000000000..3b891e2065\n--- /dev/null\n+++ b/tests/test_secret_key_fallback.py\n@@ -0,0 +1,49 @@\n+# tests/test_secret_key_fallback.py\n+import pytest\n+from django.test import Client, override_settings\n+from django.contrib.auth import get_user_model\n+from django.contrib.sessions.models import Session\n+from django.urls import path, include\n+from django.http import HttpResponse\n+\n+def _index(request):\n+    # If auth.get_user accepted the session, request.user.is_authenticated should be True\n+    assert request.user.is_authenticated\n+    return HttpResponse(\"ok\")\n+\n+urlpatterns = [\n+    path(\"\", _index, name=\"index\"),\n+]\n+\n+@pytest.mark.django_db\n+def test_secret_key_fallback_upgrades_session(settings, client, django_db_blocker):\n+    User = get_user_model()\n+    old_key = \"old-secret-key-for-test-123\"\n+    new_key = \"new-secret-key-for-test-456\"\n+\n+    # Step 1: create user and session under the old key\n+    settings.SECRET_KEY = old_key\n+    settings.SECRET_KEY_FALLBACKS = []\n+    user = User.objects.create_user(username=\"u1\", password=\"pwd123\")\n+    c = Client()\n+    assert c.login(username=\"u1\", password=\"pwd123\"), \"login failed under old key\"\n+\n+    # Pull the session row created by the login\n+    session_cookie = c.cookies[settings.SESSION_COOKIE_NAME].value\n+    session = Session.objects.get(session_key=session_cookie)\n+    decoded = session.get_decoded()\n+    assert decoded.get(\"_auth_user_hash\") == user.get_session_auth_hash()\n+\n+    # Step 2: simulate upgrade: set new key and add old key to fallbacks\n+    settings.SECRET_KEY = new_key\n+    settings.SECRET_KEY_FALLBACKS = [old_key]\n+\n+    # Ensure the client sends the same cookie and performs a request that runs AuthenticationMiddleware\n+    c.cookies[settings.SESSION_COOKIE_NAME] = session_cookie\n+    # Perform a request against the simple view; AuthenticationMiddleware will run.\n+    response = c.get(\"/\")  # ensure your test runner uses this module's urlpatterns or adjust ROOT_URLCONF\n+    assert response.status_code == 200\n+\n+    # Verify the session row has been upgraded to the new-key-derived hash\n+    session.refresh_from_db()\n+    assert session.get_decoded().get(\"_auth_user_hash\") == user.get_session_auth_hash()"
  },
  "sphinx-doc__sphinx-7590": {
    "model_name_or_path": "gpt-5-mini",
    "instance_id": "sphinx-doc__sphinx-7590",
    "model_patch": "diff --git a/sphinx/domains/cpp.py b/sphinx/domains/cpp.py\nindex fe52d881d..5d32ff4bd 100644\n--- a/sphinx/domains/cpp.py\n+++ b/sphinx/domains/cpp.py\n@@ -820,8 +820,6 @@ class ASTBooleanLiteral(ASTLiteral):\n     def describe_signature(self, signode: TextElement, mode: str,\n                            env: \"BuildEnvironment\", symbol: \"Symbol\") -> None:\n         signode.append(nodes.Text(str(self)))\n-\n-\n class ASTNumberLiteral(ASTLiteral):\n     def __init__(self, data: str) -> None:\n         self.data = data\n@@ -862,9 +860,39 @@ class ASTCharLiteral(ASTLiteral):\n         assert prefix in _id_char_from_prefix\n         self.type = _id_char_from_prefix[prefix]\n         decoded = data.encode().decode('unicode-escape')\n-        if len(decoded) == 1:\n-            self.value = ord(decoded)\n-        else:\n+class ASTUserDefinedLiteral(ASTLiteral):\n+    \"\"\"Represents a literal followed immediately by a user-defined literal suffix.\n+\n+    literal: an ASTLiteral instance (number, string, or char)\n+    suffix: an ASTIdentifier instance describing the UDL suffix\n+    \"\"\"\n+    def __init__(self, literal: ASTLiteral, suffix: \"ASTIdentifier\") -> None:\n+        self.literal = literal\n+        self.suffix = suffix\n+\n+    def _stringify(self, transform: StringifyTransform) -> str:\n+        return self.literal._stringify(transform) + str(self.suffix)\n+\n+    def get_id(self, version: int) -> str:\n+        # compose the inner literal id with a marker and the suffix id\n+        return \"%s_UD_%s\" % (self.literal.get_id(version), self.suffix.get_id(version))\n+\n+    def describe_signature(self, signode: TextElement, mode: str,\n+                           env: \"BuildEnvironment\", symbol: \"Symbol\") -> None:\n+        txt = str(self.literal) + str(self.suffix)\n+        signode.append(nodes.Text(txt, txt))\n+\n+    def _stringify(self, transform: StringifyTransform) -> str:\n+        return self.literal._stringify(transform) + str(self.suffix)\n+\n+    def get_id(self, version: int) -> str:\n+        # compose the inner literal id with a marker and the suffix id\n+        return \"%s_UD_%s\" % (self.literal.get_id(version), self.suffix.get_id(version))\n+\n+    def describe_signature(self, signode: TextElement, mode: str,\n+                           env: \"BuildEnvironment\", symbol: \"Symbol\") -> None:\n+        txt = str(self.literal) + str(self.suffix)\n+        signode.append(nodes.Text(txt, txt))\n             raise UnsupportedMultiCharacterCharLiteral(decoded)\n \n     def _stringify(self, transform: StringifyTransform) -> str:\n@@ -4619,29 +4647,47 @@ class DefinitionParser(BaseParser):\n     @property\n     def id_attributes(self):\n         return self.config.cpp_id_attributes\n+for regex in [float_literal_re, binary_literal_re, hex_literal_re,\n+                      integer_literal_re, octal_literal_re]:\n+            pos = self.pos\n+            if self.match(regex):\n+                # consume integer/float suffix characters like U, L, f, etc.\n+                while self.current_char in 'uUlLfF':\n+                    self.pos += 1\n+                # create the number literal AST node\n+                num_node = ASTNumberLiteral(self.definition[pos:self.pos])\n+                # if an identifier immediately follows (no intervening whitespace),\n+                # this is a user-defined literal suffix (UDL)\n+                if self.match(identifier_re):\n+                    suffix = ASTIdentifier(self.matched_text)\n+                    return ASTUserDefinedLiteral(num_node, suffix)\n+                return num_node\n \n-    @property\n-    def paren_attributes(self):\n-        return self.config.cpp_paren_attributes\n+        string = self._parse_string()\n+        if string is not None:\n+            str_node = ASTStringLiteral(string)\n+            # UDL suffix must immediately follow the string literal (no whitespace)\n+            if self.match(identifier_re):\n+                suffix = ASTIdentifier(self.matched_text)\n+                return ASTUserDefinedLiteral(str_node, suffix)\n+            return str_node\n \n-    def _parse_string(self) -> str:\n-        if self.current_char != '\"':\n-            return None\n-        startPos = self.pos\n-        self.pos += 1\n-        escape = False\n-        while True:\n-            if self.eof:\n-                self.fail(\"Unexpected end during inside string.\")\n-            elif self.current_char == '\"' and not escape:\n-                self.pos += 1\n-                break\n-            elif self.current_char == '\\\\':\n-                escape = True\n-            else:\n-                escape = False\n-            self.pos += 1\n-        return self.definition[startPos:self.pos]\n+        # character-literal\n+        if self.match(char_literal_re):\n+            prefix = self.last_match.group(1)  # may be None when no prefix\n+            data = self.last_match.group(2)\n+            try:\n+                char_node = ASTCharLiteral(prefix, data)\n+            except UnicodeDecodeError as e:\n+                self.fail(\"Can not handle character literal. Internal error was: %s\" % e)\n+            except UnsupportedMultiCharacterCharLiteral:\n+                self.fail(\"Can not handle character literal\"\n+                          \" resulting in multiple decoded characters.\")\n+            # character UDL suffix must immediately follow the character literal\n+            if self.match(identifier_re):\n+                suffix = ASTIdentifier(self.matched_text)\n+                return ASTUserDefinedLiteral(char_node, suffix)\n+            return char_node\n \n     def _parse_literal(self) -> ASTLiteral:\n         # -> integer-literal"
  },
  "django__django-13810": {
    "model_name_or_path": "gpt-5-mini",
    "instance_id": "django__django-13810",
    "model_patch": "diff --git a/dummy_middleware_snippet.py b/dummy_middleware_snippet.py\nnew file mode 100644\nindex 0000000000..4c8191bf25\n--- /dev/null\n+++ b/dummy_middleware_snippet.py\n@@ -0,0 +1,33 @@\n+from django.utils.decorators import sync_and_async_middleware\n+from django.core.exceptions import MiddlewareNotUsed\n+from asgiref.sync import sync_to_async\n+import asyncio\n+from django.conf import settings\n+\n+def _sync_middleware_body(request, get_response):\n+    # Place your existing synchronous middleware logic here.\n+    # It must call and return get_response(request) when delegating.\n+    return get_response(request)\n+\n+@sync_and_async_middleware\n+def dummy_middleware(get_response):\n+    # Construction-time checks: raise MiddlewareNotUsed here if the middleware\n+    # should not be applied so Django removes it cleanly during load.\n+    if getattr(settings, \"MY_MIDDLEWARE_DISABLED\", False):\n+        raise MiddlewareNotUsed(\"disabled in settings\")\n+\n+    # Async path: adapt the sync body to run in a thread if get_response is async.\n+    if asyncio.iscoroutinefunction(get_response):\n+        async def middleware(request):\n+            # Run the synchronous middleware body in a thread when it touches\n+            # sync-only internals (ORM, sessions, etc.).\n+            return await sync_to_async(_sync_middleware_body, thread_sensitive=True)(request, get_response)\n+            # If you prefer to skip this middleware for async requests, use:\n+            # return await get_response(request)\n+        return middleware\n+\n+    # Sync path:\n+    def middleware(request):\n+        return _sync_middleware_body(request, get_response)\n+\n+    return middleware"
  },
  "pytest-dev__pytest-7490": {
    "model_name_or_path": "gpt-5-mini",
    "instance_id": "pytest-dev__pytest-7490",
    "model_patch": "Agent reached maximum steps (100) without completing the task.\n\nNo changes detected to generate a patch."
  },
  "sphinx-doc__sphinx-9230": {
    "model_name_or_path": "gpt-5-mini",
    "instance_id": "sphinx-doc__sphinx-9230",
    "model_patch": "Agent reached maximum steps (100) without completing the task.\n\nNo changes detected to generate a patch."
  },
  "django__django-13297": {
    "model_name_or_path": "gpt-5-mini",
    "instance_id": "django__django-13297",
    "model_patch": "Agent reached maximum steps (100) without completing the task.\n\nNo changes detected to generate a patch."
  },
  "sympy__sympy-17655": {
    "model_name_or_path": "gpt-5-mini",
    "instance_id": "sympy__sympy-17655",
    "model_patch": "Agent reached maximum steps (100) without completing the task.\n\nNo changes detected to generate a patch."
  }
}