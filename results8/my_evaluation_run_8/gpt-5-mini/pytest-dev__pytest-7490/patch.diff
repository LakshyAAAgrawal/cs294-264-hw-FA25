diff --git a/src/_pytest/skipping.py b/src/_pytest/skipping.py
index 335e10996..832ff13a5 100644
--- a/src/_pytest/skipping.py
+++ b/src/_pytest/skipping.py
@@ -191,12 +191,6 @@ def evaluate_skip_marks(item: Item) -> Optional[Skip]:
 class Xfail:
     """The result of evaluate_xfail_marks()."""
 
-    reason = attr.ib(type=str)
-    run = attr.ib(type=bool)
-    strict = attr.ib(type=bool)
-    raises = attr.ib(type=Optional[Tuple["Type[BaseException]", ...]])
-
-
 def evaluate_xfail_marks(item: Item) -> Optional[Xfail]:
     """Evaluate xfail marks on item, returning Xfail if triggered."""
     for mark in item.iter_markers(name="xfail"):
@@ -209,6 +203,11 @@ def evaluate_xfail_marks(item: Item) -> Optional[Xfail]:
             conditions = (mark.kwargs["condition"],)
 
         # Unconditional.
+        if not conditions:
+
+            xfail("[NOTRUN] " + xfailed.reason)
+
+    yield
         if not conditions:
             reason = mark.kwargs.get("reason", "")
             return Xfail(reason, run, strict, raises)
@@ -235,9 +234,13 @@ def pytest_runtest_setup(item: Item) -> None:
 
     skipped = evaluate_skip_marks(item)
     if skipped:
-        item._store[skipped_by_mark_key] = True
-        skip(skipped.reason)
-
+@hookimpl(hookwrapper=True)
+def pytest_runtest_call(item: Item) -> Generator[None, None, None]:
+    # Execute the test call first, then re-evaluate xfail marks to pick up markers
+    # added during the test execution (e.g., via request.node.add_marker).
+    outcome = yield
+    xfailed = evaluate_xfail_marks(item)
+    item._store[xfailed_key] = xfailed
     if not item.config.option.runxfail:
         item._store[xfailed_key] = xfailed = evaluate_xfail_marks(item)
         if xfailed and not xfailed.run: